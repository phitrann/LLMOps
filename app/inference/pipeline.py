# from app.inference.model import llm_model

# def run_inference(input_text: str) -> str:
#     # Define the conversation or messages to be passed to the model
#     messages = [
#         {"role": "system", "content": "You are an AI assistant that helps people find information. Answer questions using a direct style. Do not share more information that the requested by the users."},
#         {"role": "user", "content": input_text},
#     ]

#     # Generate the response using the LLMModel instance
#     response = llm_model.generate(messages)
    
#     return response
